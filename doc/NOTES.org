* Things to do
** DONE Fix accidentals/DMC calc and compare 1+ daily value to LBNL [3/4]
CLOSED: [2020-01-15 Wed 23:30]
- [X] Ensure singles sel/calc use consistent muon veto
- [X] Apply stashed corrections to calc
Still not fully consistent with technote, see (1.10) [FIXED]
- [X] Process one day in each hall, compare to LBNL
Should check more extensively?
- [ ] Fix acc rate error? [WONTFIX]
** DONE Implement Li9 rate calculation [1/2]
CLOSED: [2020-01-15 Wed 23:30]
- [X] Write code
- [ ] Test code
** TODO Check implementation of other backgrounds
In fit_prep? c.f. 
** DONE Add accidentals spectrum?
CLOSED: [2020-01-16 Thu 12:17]
Already have the singles histograms, dur.
** TODO Run checks of daily everything vs LBNL
** DONE Update job script for stage1
CLOSED: [2020-01-28 Tue 17:03]
** DONE Update stage1 merge job?
CLOSED: [2020-01-28 Tue 17:03]
** TODO Write/test job script to do stage2 + merge + fit
** DONE Run stage1 (+ merge?) on P17B
CLOSED: [2020-01-28 Tue 17:03]
** TODO Do one fit, to test
** TODO Do another N fits

* Usage with PyROOT
...(for, e.g., using test_candidates.py)
With current janky environment, do not compile C++ from within PyROOT! The "wrong" dynamic libs get linked in. Compile code from "plain" ROOT, then load the .so from PyROOT. This applies to things like test/FileFinder.cc too.

* PyROOT bugs
- [[https://sft.its.cern.ch/jira/browse/ROOT-7240][Scoped enums]]

* Multiple input files?
Problem is in stage1: AdSaver needs to be able to know when the input file has changed so that it can update the run/file in the output tree. SyncReader needs to be able to notify downstream algs when the input file changes to the next one in the chain. Stage2 is fine as-is.

* Environment setup
Start with a fresh login. Then
#+begin_src bash
source bash/job_env.inc.sh
#+end_src
Now you are in the same environment that jobs will run in. Includes Python 3.7, ROOT 6.18, Pandas. It's OK to submit jobs from a different environment, as we whitelist the env vars that get exported to the job. However, for doing things interactively, it's a good idea to use the job environment.

* Testing stage1
** Generating smaller input
#+begin_src bash
# First 10 files:
scripts/prep_stage1.sh -f "head -n -10" $tag

# Random 10 files:
scripts/prep_stage1.sh -f "shuf -n 10" $tag
#+end_src

** Running interactively (one process)
#+begin_src bash
newenv
bash/stage1_job.sh $tag
#+end_src

** Checking sbatch command
Set IBDSEL_DRYRUN=1 when running submit_stage1_foo.sh

** Testing on batch
#+begin_src bash
# $sys is either knl or hsw
tests/submit_stage1_debug_$sys.sh $tag
#+end_src

** Cleaning up
eval `scripts/clear.sh stage1 $tag`

* Submitting stage1
Do everything from ibd_prod directory, within a fresh shell environment

** Safety check
Make sure stage1_main.cc.so is the newest file in selector/, and be sure that you didn't compile it from PyROOT

** Prepare dirs
#+begin_src bash
scripts/prep_stage1.sh $tag
#+end_src

** Submit
#+begin_src bash
scripts/submit_stage1_$sys.sh $tag $njob
#+end_src

** Iterating to completion (not tested)
If no jobs are running:
#+begin_src bash
scripts/filter_done.sh stage1 $tag
#+end_src

If N jobs are running, calculate pending = N * chunksize, then
#+begin_src bash
scripts/filter_done.sh -p $pending stage1 $tag
#+end_src
This assumes that all running jobs are processing items drawn from the current version of input.stage1.txt. If that file was updated after a job was launched, and that job has yet to pull any items off the new list, then the above method won't work right. Some files will be omitted even though nobody is processing them, and some in-progress files will be include. If in doubt, just wait for jobs to finish.

** End result
560k stage1 files in ../../data/stage1_fbf/$tag/EH1/0021200/0021221 etc.

* Merging stage1
** Prepare input
#+begin_src bash
scripts/prep_merge1.sh $tag
#+end_src

** Running the merge
#+begin_src bash
scripts/run_merge1.sh $tag $nproc
#+end_src
It's fine to add processes to a running merge. With 8 processes spread between two Cori login nodes, P17B took 4 hours.

** Iterating (not tested)
#+begin_src bash
scripts/filter_done.sh merge1 $tag
#+end_src

** Checking
Grep the logs for CRAPPY. Try to redo stage1 for those files. Those that cannot be resolved should be tagged as bad. (P17B good run list v3 should be 100% viable.)

** Cleanup
Delete the fbf files

** Preservation
Copy the dbd files to CFS

** End result
6k daily stage1 files in ../../data/stage1_dbd/$tag/EH1 etc.

* Stage2 testing
** DONE Run stage2_job.sh interactively, no IBDSEL_CONFIGDIR
CLOSED: [2020-01-31 Fri 21:44]
** DONE Run stage2_job.sh interactively, set IBDSEL_CONFIGDIR to ../misc/configs and use a "config.altered.txt" in there
CLOSED: [2020-01-31 Fri 23:17]
** DONE Test in debug QOS
CLOSED: [2020-02-01 Sat 01:41]
** TODO Using performance numbers, draft submit_stage2_${sys}_debug.sh
** TODO Run some benchmarks to determine ideal performance params
** TODO Update submit_XXX performance params
** TODO Investigate "MuonAlg is behind!" etc.
** TODO Investigate discrepancy in number of days between halls
** TODO Add any needed branches to the IBD trees
** TODO Submit one full job

* Submitting stage2
** Preparing
#+begin_src bash
# if unset, uses ../static/configs (which just has nominal cuts)
export IBDSEL_CONFIGDIR=some_dir
# use -f to produce a short list for testing purposes, like prep_stage1
scripts/prep_stage2.sh $tag $configname
#+end_src

** Clearing
#+begin_src bash
eval $(scripts/clear.sh stage2 $tag $configname)
#+end_src
See also reset.sh

** Testing interactively
#+begin_src bash
bash/stage2_job.sh $tag $configname
#+end_src

** Submitting
#+begin_src bash
scripts/submit_stage2_knl.sh $tag $configname 10
#+end_src

** Iterating

** End result
6k daily stage2 files 
